{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LightningLM",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xghZnMUnqqb6HwXrqCwH9Gh5J2KHjLNN",
      "authorship_tag": "ABX9TyPOuuLx2sVkiiqF9b56U0Yv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashmehra/AntColonyOptimization/blob/master/LightningLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiKztaVCGt9z"
      },
      "source": [
        "### Install PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6alK1tGhOc",
        "collapsed": true
      },
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install wandb -qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9UQzanrHLvr"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import  Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy3eYsZCIc2Z",
        "collapsed": true
      },
      "source": [
        "wandb.login(key='') # set key here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbwukZqIQXhd",
        "collapsed": true
      },
      "source": [
        " wandb.init(\n",
        "      # Set entity to specify your username or team name\n",
        "      # ex: entity=\"carey\",\n",
        "      # Set the project where this run will be logged\n",
        "      entity = \"modai\",\n",
        "      project=\"nlp_tutorials\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"learning_rate\": 1e-2,\n",
        "      \"architecture\": \"LSTM\",\n",
        "      \"dataset\": \"WikiText-2\",})\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjAZJgF7QBcx"
      },
      "source": [
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "BATCH_SIZE = 24 if AVAIL_GPUS else 12\n",
        "NUM_WORKERS = int(os.cpu_count() / 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJVHkP-IQPmB"
      },
      "source": [
        "wandb_logger = WandbLogger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hkArHkCLNJd"
      },
      "source": [
        "### Lightning Model\n",
        "Here we define the model using PyTorch Lightning's self-contained approach. The model is a very simple **Language Model** which is an `LSTM Encoder` followed by a `Fully Connected Layer` and a `Softmax`. We output the distribution over the tokens to predict the next one given the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6iDb9S2H2ce"
      },
      "source": [
        "class RNNLanguageModel(pl.LightningModule):\n",
        "    def __init__(self, input_size: int, hidden_size: int = 64, \n",
        "                 dropout_prob: float = 0.4, num_layers: int = 2,\n",
        "                 tie_weights: bool = True):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, \n",
        "                           num_layers=num_layers, bias=False, batch_first=True, \n",
        "                           bidirectional=False)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.decoder = nn.Linear(hidden_size, input_size)\n",
        "        if tie_weights:\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        self.rnn.flatten_parameters()\n",
        "        output, hidden = self.rnn(encoded)\n",
        "        return self.decoder(output)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x,y = batch['sources'], batch['targets']\n",
        "        embedded = self.encoder(x)\n",
        "        self.rnn.flatten_parameters()\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        hx, ctx = hidden\n",
        "        output = self.dropout(output)\n",
        "        output = output.reshape(output.size(0) * output.size(1), output.size(2))\n",
        "        x_hat = self.decoder(output)\n",
        "        loss = F.cross_entropy(x_hat, y.reshape(-1))\n",
        "        self.log(\"training_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self, lr=1e-2):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        return optimizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Whm2E8RKaQ"
      },
      "source": [
        "class ToTensor():\n",
        "    def __call__(self, sample):\n",
        "        sample['source'] = torch.LongTensor(sample['source'])\n",
        "        sample['target'] = torch.LongTensor(sample['target'])\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZZ74zpFJORx"
      },
      "source": [
        "class Dictionary(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.dont_care = 0\n",
        "        self.bos_token = 1\n",
        "        self.eos_token = 2\n",
        "        self.unk_token = 3\n",
        "        self.bos_word = '<bos>'\n",
        "        self.eos_word = '<eos>'\n",
        "        self.unk_word = '<unk>'\n",
        "        \n",
        "        self._word2idx = {self.bos_word: self.bos_token, \n",
        "                          self.eos_word: self.eos_token, \n",
        "                          self.unk_word: self.unk_token}\n",
        "        \n",
        "        self._idx2word = [self.dont_care, self.bos_word, self.eos_word, self.unk_word]\n",
        "        self._dist = Counter()\n",
        "        \n",
        "    def add_word(self, word):\n",
        "        self._dist[word] += 1\n",
        "        if word not in self._word2idx:\n",
        "            self._idx2word.append(word)\n",
        "            self._word2idx[word] = len(self._idx2word) - 1\n",
        "        #assert len(self._dist) == len(self._idx2word) == len(self._word2idx)\n",
        "        return self._word2idx[word]\n",
        "\n",
        "    def idx_to_word(self, idx):\n",
        "        return self._idx2word[idx]\n",
        "    \n",
        "    def word_to_idx(self, word):\n",
        "        return self._word2idx[word]\n",
        "    \n",
        "    def __add__(self, other):\n",
        "        pass\n",
        "        \n",
        "    def __iadd__(self, other):\n",
        "        widmap = other.wordidmap\n",
        "        for k,v in widmap.items():\n",
        "            if k not in self._word2idx:\n",
        "                self._word2idx[k] = len(self._word2idx)\n",
        "                self._idx2word.append(k)\n",
        "        return self\n",
        "    \n",
        "    @property\n",
        "    def wordidmap(self):\n",
        "        return self._word2idx\n",
        "    \n",
        "    @property\n",
        "    def idxwordmap(self):\n",
        "        return self._idx2word\n",
        "    \n",
        "    @property\n",
        "    def word_count(self):\n",
        "        return self._dist\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._idx2word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETQuDOuwPKOG"
      },
      "source": [
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, path: str, dictionary: Dictionary, transform = ToTensor()):\n",
        "        self._dictionary = dictionary\n",
        "        self._tokens = self._tokenize(path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def _tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        doc = []\n",
        "        tokens = []\n",
        "        self._num_tokens = 0\n",
        "        # read lines from file.\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                if len(line.strip()) == 0:\n",
        "                    continue\n",
        "                words = [word.lower() for word in line.strip().split()]\n",
        "                self._num_tokens += len(words)\n",
        "                doc.append(words)\n",
        "                \n",
        "        for words in tqdm(doc): \n",
        "            tokens.append([self._dictionary.word_to_idx(word) for word in words])\n",
        "                \n",
        "        return tokens\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._tokens)\n",
        "    \n",
        "    @property\n",
        "    def corpus_size(self):\n",
        "        return self._num_tokens\n",
        "    \n",
        "    def numpy(self):\n",
        "        samples = []\n",
        "        transform = ToNumpy()\n",
        "        for idx in range(len(self._tokens)):\n",
        "            sample = self[idx]\n",
        "            sample = transform(sample)\n",
        "            samples.append(sample)\n",
        "        return samples\n",
        "    \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        return self._dictionary\n",
        "        \n",
        "    def sentence(self, idx):\n",
        "        return ' '.join([self._dictionary.idx_to_word(token) for token in self._tokens[idx]])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self._tokens[idx]\n",
        "        sample = {\n",
        "            'source': [self._dictionary.bos_token] + tokens,\n",
        "            'target': tokens + [self._dictionary.eos_token]\n",
        "        }\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rLeFCIcPEEw"
      },
      "source": [
        "def seq_collate_fn(data):\n",
        "    \n",
        "    def padding(seqs, seq_lens, dtype=torch.LongTensor):\n",
        "        batch_size = len(seqs)\n",
        "        max_seq_len = max(seq_lens)\n",
        "        source_padded = torch.zeros(batch_size, max_seq_len).type(dtype)\n",
        "        target_padded = torch.zeros(batch_size, max_seq_len).type(dtype)\n",
        "        for i, seq in enumerate(seqs):\n",
        "            end = len(seq['source'])\n",
        "            source_padded[i,:end] = seq['source'] \n",
        "            target_padded[i,:end] = seq['target']\n",
        "        \n",
        "        return source_padded, target_padded\n",
        "        \n",
        "    data.sort(key=lambda d: len(d['source']), reverse=True)\n",
        "    seq_lens = [len(d['source']) for d in data]\n",
        "    source_padded, target_padded = padding(data, seq_lens)\n",
        "    samples = {\n",
        "        'sources': source_padded,\n",
        "        'targets': target_padded,\n",
        "        'seq_lens': seq_lens,\n",
        "    }\n",
        "    \n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMq6XPjKzoR"
      },
      "source": [
        "class SentenceDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, path: str, batch_size: int, num_workers: int):\n",
        "        super().__init__()\n",
        "        self.data_path = path\n",
        "        self.batch_size = batch_size \n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        self.train_path = os.path.join(path, 'train.txt')\n",
        "        self.val_path = os.path.join(path, 'valid.txt')\n",
        "        self.test_path = os.path.join(path, 'test.txt')\n",
        "        self.all_paths = [self.train_path, self.val_path, self.test_path]\n",
        "        self.transforms = ToTensor()\n",
        "        self.vocab = self._create_vocab(self.all_paths)\n",
        "        self.dims = len(self.vocab)\n",
        "        \n",
        "    \n",
        "    def prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage = None):\n",
        "        print(f\"In Setup, with stage: {stage}\")\n",
        "        #if stage == 'fit' or stage is None:\n",
        "        self.train_dataset = SentenceDataset(self.train_path, self.vocab, \n",
        "                                                 transform=self.transforms)\n",
        "        self.val_dataset = SentenceDataset(self.val_path, self.vocab,\n",
        "                                               transform=self.transforms)\n",
        "        #if stage == 'test' or stage is None:\n",
        "        self.test_dataset = SentenceDataset(self.test_path, self.vocab,\n",
        "                                                transform=self.transforms)\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
        "                          num_workers=self.num_workers, collate_fn=seq_collate_fn)\n",
        "        \n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, \n",
        "                          num_workers=self.num_workers, collate_fn=seq_collate_fn)\n",
        "        \n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, \n",
        "                          num_workers=selfnum_workers, collate_fn=seq_collate_fn)\n",
        "\n",
        "    def _create_vocab(self, paths):\n",
        "        vocab = Dictionary()\n",
        "        for path in paths:\n",
        "            # Add words to the dictionary\n",
        "            with open(path, 'r', encoding=\"utf8\") as f: \n",
        "                for line in f:\n",
        "                    if len(line.strip()) == 0:\n",
        "                        continue\n",
        "                    words = [word.lower() for word in line.strip().split()]\n",
        "                    for word in words:\n",
        "                        vocab.add_word(word)\n",
        "        return vocab\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ct2x5S7QJQc"
      },
      "source": [
        "dm = SentenceDataModule('drive/My Drive/wiki-2', BATCH_SIZE, NUM_WORKERS)\n",
        "lm = RNNLanguageModel(dm.size(), 256, dropout_prob=0.2, num_layers=4)\n",
        "trainer = pl.Trainer(gpus=AVAIL_GPUS, max_epochs=5, progress_bar_refresh_rate=2, logger=wandb_logger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOMEZINDSIO2"
      },
      "source": [
        "trainer.fit(lm, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEzNMQABkLP7",
        "collapsed": true
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnr7XXhcSWzM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}